\documentclass[a4paper,10pt]{llncs}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsbsy,amscd,amsfonts,amssymb,amstext,amsmath,latexsym,theorem}
\usepackage{todonotes}
\usepackage{bussproofs}
\usepackage{nameref}
\usepackage{url}

\pagestyle{plain}
\bibliographystyle{alpha}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{{\normalsize Seminar: Formal Specification} \\[1ex]
  Security of Multithreaded Programs by Compilation\cite{Barthe07}}
\author{Pascal Wittmann, Advisor: Artem Starostin}
\institute{TU Darmstadt}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}
\label{sec:motivation}
Barthe, Rezk, Russo and Sabelfeld introduced with
their paper \cite{Barthe07} a framework to guarantee
noninterference in multithreaded programs at byte-code
level. Noninterference is a security property which
says that a program does not leak sensitive information
to an adversary. Since more and more (mobile) devices handle
an increasing number of tasks, multithreading is widely used
to prevent lock ups (when e.g. establishing a
network-connection). But often programs that are secure when they
are executed sequentially, leak information when they are
composed in parallel. This leads to an attractive channel of
information leakage.

The idea of Barthe et. al. was to close this channel by compiling
type information into the byte-code, which cause the scheduler to
treat the secure parts in a different way. The fact that many mobile
platforms already use some byte-code (e.g. Android uses a modification
of the JVM byte-code) may have been a reason for the decision to use
type-annotated byte-code.

\section{Introduction to research area}
\label{sec:introduction}
\section{Summary of the article}
\label{sec:discussion}
In this summary I will follow mainly the structure of the original
paper\cite{Barthe07} which is as follows. After the introduction the basic
terms and definitions for a multithreaded programs and the scheduler are
laid. After that the notion of security we want to achieve is presented.
Along with this a skeleton of a type system is described, which ensures
that a program typable in this type system is secure w.r.t the notion of
security. The proof that this holds is sketched in the following section.
In the last section the (by now) abstract framework is instantiated with a
concrete example.

\subsection{Syntax and Semantics of multithreaded programs}
\label{sec:syntaxsemantics}
A program is viewed as an abstract thing, which consists of a set of
program points $\mathcal{P}$ with a distinguished entry ($1$) and exit
(\texttt{exit}) point and a function $P$ that maps program points to
instructions.

These instructions are not further specified, but contain an instruction
to create a new thread (\texttt{start \textit{pc}} where pc is the start
instruction of the new thread).

Further, there is a relation $\mapsto$ that describes possible successor
instructions. \texttt{exit} is the only program point with no successor
and \texttt{start \textit{pc}} may only have a single successor (the
following program point).

The next thing introduced are the security levels. We assume the attacker
'is' a level $k$. From this assumption we can reduce every set of levels
w.l.o.g into $\{low, high\}$, where $low < high$, by mapping elements
that are no more sensitive than $k$ to $low$ and all other elements
-- including incomparable ones -- to $high$.

To connect programs and security levels, a \textit{security environment}
(se) is defined, which is used to prevent flows over implicit channels.
A security environment is a function that maps program points to security
levels. A program point $i$ is called high if $se(i) = high$, low if $se(i)
= low$ and always high if all points $j$ reachable (according to $\mapsto$)
from $i$ satisfy $se(j) = high$ and $i$ is a high program point.

Now we come to the semantics part. The main idea is to build the semantics
for multithreaded programs by combining the semantics for sequential programs
with a scheduler.

All active\footnote{A thread is active from \texttt{start \textit{pc}} until
it reaches the \texttt{exit} point.} threads are collected in a set $Thead$.
The state of the concurrent running threads ($ConcState$) is defined as the
product of the partial function space $(Thread \rightharpoonup LocState)$
and the set of global memories $GMemory$. Where $LocState$ is the internal
memory of a thread, from there no information can leak and the $GMemory$ is
the critical part of the system, because it is a memory shared between all
active threads.

At this point a first simplification can be made. Looking a state $s$ we can
first extract the active threads (\texttt{s.act}) by taking the domain of the first
component. According to a security environment we can classify these threads,
with respect to their current program point (\texttt{s.pc(tid)} where $tid \in Thread$),
into \textit{low} threads iff the current program point is low. In \textit{high}
threads iff the current program point is high. In \textit{always high} threads
iff the current program point is always high and in \textit{hidden} threads, iff
the current program point is high but not always high.

The latte two are the interesting ones. If a thread is \textit{always high} it
can not leaks any information into low, because it never only gets in touch
with low program points. So these threads can safely be interleaved by the
scheduler.

The \textit{hidden} threads are the ones we have to care about. These contain
obviously high information in the current program point, but have afterwards
instructions that deal with low information. Since the attacker can watch the
low part of the memory, chances are good that he can deduce high information
through changing low outputs. To prevent indirect flows that are introduced by
these hidden threads, the scheduler will be modified to treat these threads
in a special way. This will be done by 'hiding' these threads, therefore comes
the name of them.

To complete our multithreaded setup we need a scheduler. The scheduler will
operate on histories. A history is a list of pairs $(tid, l$), where $tid \in
Thread$ and $l \in Level$. In this history all threads chosen by the scheduler
are recorded.

At this point no concrete scheduler is defined to make the framework applicable
for a wide class of schedulers. A scheduler is in this class iff it can be
modeled as a function $pickt: ConcState \times History \rightharpoonup Thread$
which satisfies the following constraints:

\begin{enumerate}
\item It always picks an active thread
\item If there is a hidden thread, always choose high or always high threads
\item Only use low information and the low part of the history to choose a
      new thread
\end{enumerate}

In constraint two the interleaving of always high threads is realized.

\todo{Explain how the programs are executed}

Now we can define our goal: Noninterference. We define noninterference
in accordance to an indistinguishability relation $\sim_g$ on global memories.
Barthe et.al. state that it is not necessary, for the purpose of the paper,
to specify the definition of this relation. But to get a feel for this
relation, you can define it as: $\mu \sim_g \mu' \Leftrightarrow \mu|_{low} =
\mu'|_{low}$ where $\mu|_{low}$ projects out all high elements. Based on that
a program P is noninterfering if for all gobal memories $\mu_1, \mu_2, \mu_1'$
and $\mu_2'$ it holds that:

\begin{align*}
\mu_1 \sim_g \mu_2 \land P,\mu_1 \Downarrow \mu_1' \land P,\mu_2 \Downarrow \mu_2' \rightarrow \mu_1' \sim_g \mu_2'
\end{align*}

\subsection{Type system}
\label{sec:typesystem}
The type system is the core part of the framework in the effect that it
enforces the previously defined noninterference property. Thus every
program which is typable, is noninterfering.

The type system for multithreaded programs is build up from a type system
for sequential programs for which the following assumptions hold:

\todo{Explain intuition}
\begin{enumerate}
\item We have a partial ordered set $(LType, \leq)$ of local types, with an initial
      type $t_{init}$
\item and typing judgments of the form $se, i \vdash_{seq} s \Rightarrow t$, where
      $s, t \in LType$, $i \in \mathcal{P}$ and se is a securirty environment.
\end{enumerate}

This type system is extended by the following rules, to support multithreading:

\todo{Place proofstrees into one line with minipages}
\begin{figure}
\begin{prooftree}
\AxiomC{P[i] $\in$ SeqIns}
\AxiomC{$se, i \vdash_{seq} s \Rightarrow t$}
\BinaryInfC{$se, i \vdash s \Rightarrow t$}
\end{prooftree}

\begin{prooftree}
\AxiomC{P[i] = \texttt{start} pc}
\AxiomC{$se(i) \leq se(pc)$}
\BinaryInfC{$se, i \vdash s \Rightarrow s$}
\end{prooftree}
\caption{Extension of the sequential typing rules}
\label{fig:multithreaded-typing-rules}
\end{figure}

The first rule states, that sequential commands are treated as usual
and the second rule ensures, that the security level of the entry point
of the spawned thread, is lower bounded by the level of the \texttt{start}
instruction.

A program is typeable in this type system $\mathcal{S}, se \vdash P$, where a function $\mathcal{S}:
\mathcal{P} \rightarrow LType$ that maps a local type to every program point
and a security environment $se$, iff $\mathcal{S}$ maps every initial program point
(including the ones of spawned threads) to $t_{init}$ and for every program point $j$ which is an
successor of $i$ there is a type $s \in LType$, such that $s$ is lower bounded
by $\mathcal{S}(j)$ and $se, i \vdash \mathcal{S}(i) \Rightarrow s$ holds.

\todo{Explain intuition}

\subsection{Soundness}
\label{sec:soundness}
The framework is now complete, but the proof of the connection between the
type system and the noninterference property is still outstanding. The full
proof is not part of the paper and I will only sketch the most important
parts.

The goal is to proof the following theorem:

\begin{theorem}
If the scheduler is secure and $se, \mathcal{S} \vdash P$, then P is noninterfering.
\end{theorem}

The scheduler is secure, if it is defined w.r.t. the conditions from section \ref{sec:syntaxsemantics}.

An important hypothesis to succeed in the proof of this theorem is the existence
of a \texttt{next} function. This \texttt{next} function should compute for every
high program point the first subsequent program point with a low security level. With
this function one is able to detect when a hidden thread is allowed to become visible
again.

This intuition is capture in the following properties for the function $next: \mathcal{P}
\rightharpoonup \mathcal{P}$:

\todo{Use names from paper}
\begin{enumerate}
\item $Dom(next) = \{i \in \mathcal{P} | se(i) = high \land \exists j \in \mathcal{P} . i \mapsto^* j \land se(j) \neq high\}$ \todo{use simple method of new paper?}
\item $i, j \in Dom(next) \land i \mapsto j \Rightarrow next(i) = next(j)$
\item $i \in Dom(next) \land j \not\in Dom(next) \land i \mapsto j \Rightarrow next(i) = j$
\item $j, k \in Dom(next) \land i \not\in Dom(next) \land i \mapsto j \land i \mapsto k \land j \neq k \Rightarrow next(j) = next(k)$ \todo{is there an error?}
\item $i, j \in Dom(next) \land k \not\in Dom(next) \land i \mapsto j \land i \mapsto k \land j \neq k \Rightarrow next(j) = k$
\end{enumerate}

Die domain of next just captures the high, but not always high program points (i.e. the ones
that can result in a hidden thread). The second property, states that two directly successive high
program points have the same program point in which the thread becomes visible again. The third rule
is the counterpart of the second: If a low program point follows a high program point, this low program
point is the result of the next function for the high program point.

\todo{What is wrong with property 4 (NeP3)}
\todo{What is wrong with property 5 (NeP4)}

\subsection{Instantiation}
\label{sec:instantiation}
To demonstrate how the framework can be used, it was instantiated with a simple
assembly language, which is given by the follow grammar:

\todo{beautify this}
\begin{align*}
instr &::= binop\ op && \text{binary operation with values from stack} \\
&|\ push\ n &&  \text{push value on the stack} \\
&|\ load\ x && \text{push value of variable on the stack} \\
&|\ store\ x && \text{store first element of the stack in x} \\
&|\ goto\ j\ |\ ifeq\ j && \text{un-/conditional jump to j} \\
&|\ start\ j && \text{create a new thread starting in j} \\
\end{align*}

where $op \in \{+,-,\times,/\}$, $n \in \mathbb{Z}$ and $x$ are variables. The
operational semantics are standard and not explicitly necessary for the following
instantiation, therefor they are omited.

The local states are modeled as a pair of the operand stack and the program counter. The
initial state $\lambda_{init}(pc)$ has an empty operand stack $\epsilon$ and points to
the given initial program point.

Besides this concrete language we need to define a type system to enforce noninterference
according to \ref{sec:typesystem}. The local types are defined by a stack of security levels
$LType = Stack(Level)$ and $t_{init}$ with the empty stack. The typing rules defined in
Figure~\ref{fig:multithreaded-typing-rules} need to be extended with rules for the concrete instructions
of the assembly language. In this summary I only explain two of them, the rest follows
in a similar manner.

\todo{beautify me}
\begin{figure}
\begin{prooftree}
\AxiomC{$P[i] = store\ x$}
\AxiomC{$se(i) \sqcup k \leq \Gamma(x)$}
\BinaryInfC{$se, i \vdash_{seq} k :: st \Rightarrow st$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$P[i] = ifeq\ j$}
\AxiomC{$\forall j' \in reg(i), k \leq se(j')$}
\BinaryInfC{$se, i \vdash_{seq} k :: st \Rightarrow lift_k(st)$}
\end{prooftree}
\caption{Excerpt of typing rules for the assembly language}
\label{fig:transfer-rules}
\end{figure}

The security levels in $LType$ are the security levels of the operands in the local state. To
express our security policy, we declare a function $\Gamma(x)$ that assigns to every variable
$x$ a security level. If we now want to \texttt{store} the top of the stack into the variable
$x$ one premise is that the security level of the program point ($se(i)$) \textit{and} the
security level of the value on top of the operand stack is \textit{lower or equal} to the
security level that was assigned by $\Gamma$ to the variable $x$. If this is the case the head
of $LType$ is removed.

The rule for the branching instruction can only be used if the security level of all program
points in the control dependence region are lower bounded by the security level of the value
that is used as the condition (i.e. the top of the operand stack). \todo{What does $lift_k$?}

The next step is to construct the \texttt{next} function. To make this task easy, we introduce
a source language which will be complied into the given assembly language. The source language
is defined as follows:

\begin{align*}
e ::= n\ |\ x\ |\ e\ op\ e && c\ ::=\ x\ :=\ e\ |\ c;c\ |\ if\ e\ then\ c\ else\ c\ |\ while\ e\ do\ c\ |\ fork(c)
\end{align*}

Now it is easy to define the control dependence region and the junction points for the source
language and deduce the ones for the assembly language from them. The first step is to label the
source language at the regions at risk with natural numbers:

\begin{align*}
c\ ::=\ [x\ :=\ e]^n\ |\ c;c\ |\ [if\ e\ then\ c\ else\ c]^n\ |\ [while\ e\ do\ c]^n\ |\ [fork(c)]^n
\end{align*}

According to this labels we can define the dependence regions for the source language:

\begin{definition}{sregion}
The control dependence region for a branching command $[c]^n$ in the source language is
defined as the labels inside the branching command, except for those ones that are inside a
\texttt{fork} command.
\end{definition}

Now we can define $tregion$ according to a compilation function $\mathcal{C}$. I won't define
the compilation function here, because it is not that essential how it exactly looks like.

\begin{definition}{tregion}
$tregion(n)$ is defined as the set of instruction labels obtained by compiling the commands
$[c']^{n'}$ of the branching instruction $[c]^n$ of the source code with a compilation function $\mathcal{C}$.
\end{definition}

With this definition of $tregion$ in mind, it is easy to define the junction points.

\begin{definition}{jun}
The junction points are computed by a function $jun: \mathcal{P} \rightarrow \mathcal{P}$. It is defined
on all junction points $[c]^n$ in the source program and its domain is $jun(n) = max \{i\ |\ i \in tregion(n)\} + 1$.
\end{definition}

Intuitively a junction point according to this definition is the point that follows on the last instruction
that is affected by the branching instruction.

This looks familiar to the \texttt{next} function we want to define.\footnote{We could define \texttt{next} for
every instruction $i$ inside an outermost branching point $[c]^n$ as $next(i) = jun(n)$.}. The part that is
missing, is the restriction to outermost branching points whose guards involves secrets.

To make this distinction a new type system is introduced. Type judgments have the form
$\vdash_\alpha [c]_{\alpha'}^n : E$, where $E$ is a function that maps labels to security
levels.\footnote{Given $E$ it is easy to define a security environment $se$. For a definition
of $E$ see \cite{Barthe06}.} $\alpha$ denotes if $c$ is \textit{part of} a branching
instruction that branches on secret ($\bullet$) or public ($\circ$) data and $\alpha'$ is the
security level of the guard \textit{in} the branching instruction.

We now take a look at the rules of this type system concerning the \texttt{if} instruction.

\todo{use an other package to display these rules}
\begin{figure}
\begin{prooftree}
\AxiomC{$\vdash e : L$}
\AxiomC{$\vdash_\alpha c : E$}
\AxiomC{$\vdash_\alpha c' : E$}
\LeftLabel{L-COND:\quad}
\TrinaryInfC{$\vdash_\alpha [if \ e \ then \ c \ else \ c']^n_\alpha : E$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\vdash e : H$}
\AxiomC{$\vdash_\bullet c : E$}
\AxiomC{$\vdash_\bullet c' : E$}
\LeftLabel{H-COND:\quad}
\TrinaryInfC{$\vdash_\bullet [if \ e \ then \ c \ else \ c']^n_\bullet : E$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\vdash e : H$ \quad $\vdash_\bullet c : E$}
\AxiomC{$\vdash_\bullet c' : E$}
\AxiomC{$E = lift_H(E, sregion(n))$}
\LeftLabel{TOP-H-COND:\quad}
\TrinaryInfC{$\vdash_\circ [if \ e \ then \ c \ else \ c']^n_\bullet : E$}
\end{prooftree}

\caption{Typing rules for \texttt{if} on source level.}
\label{fig:typing-rules-if}
\end{figure}

The first rule L-COND from Figure \ref{fig:typing-rules-if} says that if we branch on a low
guard, everything depends on the security level of $c$ and $c'$. The rule H-COND covers the
case where we have a high guard, in this case the control dependence region has to be marked
high. The rule TOP-H-COND is the interesting one. Because of the preceding rules we can not
be part of branch with a low guard, therefore we are in the outermost high branch. The premise
$E' = lift_H(E, sregion(n))$ reads as: For all labels in the control dependence region $E$ is
defined as $E'(n) = H \sqcup E(n)$ and $E'(n) = E(n)$ for all other labels.

This type system is powerful enough to prevent explicit and implicit flows and can therefor
replace the type system defined previously. \todo{show ASSIGN rule?}

With this type system in mind, we can now define our \texttt{next} function.

\begin{definition}{next}
For every branching point $c$ in the source program such that $\vdash_\circ [c]_\bullet^n$, we  have
that $\forall k \in tregion(n) . next(k) = jun(n)$.
\end{definition}

The proof that this definition fulfills the properties from section \ref{sec:soundness} and all other
proofs can be found in \cite{Barthe09}.

Now the instantiation of the framework is complete, except for the scheduler which was left undefined
in the paper.

\section{Comparison with other aproaches/further work}
\label{sec:furtherwork}

\bibliography{bibliography}

\end{document}
